# Introduction


<!-- 1.4. The challenges of the Human Cell Atlas -->
## The Human Cell Atlas (HCA) Project

<!-- 1.4.1. The Human Cell Atlas project and its scope -->
<!-- 1.4.1.1. Participants -->
The advent of single-cell technologies has ignited the desire of a deep knowledge on cells, the building blocks of life [@wikidata:Q99418649]. 
The Human Cell Atlas (HCA) project, has been a major player in the cell knowledge ecosystem, running since 2017 towards the task to characterize every cell type in the human body [@wikidata:Q46368626]. The HCA consortium gathers people from all over the world to tackle different parts of the project. 

<!-- - 1.4.1.2. Overview of main analytical techniques  -->
Building a full atlas of human cells comes with multiple challenges. The project includes the detection, in single cells, of RNA species (scRNA-Seq), chromatin accessibility (scATAC-Seq), and protein markers (primarily by CYTOF), as well as spatial information on cells with multiplexed _in situ_ hybridization (such as MERFISH) and imaging mass cytometry [@wikidata:Q46368626 ;@wikidata:Q104450645]. Every lab inside the project will contribute with its expertise, providing samples that are representative of human diversity.

HCA is set to revolutionize the biomedical sciences, by creating tools and standards for basic research, as well as allowing better characterization of disease, and thus, ultimately, improving diagnostics and therapy. 
Its products (data, information, knowledge and wisdom) need to be FAIR: findable, accessible, interoperable and reusable.
Data stewardship and data management are growing as core demands of the scientific community, ranging from data management plans [@wikidata:Q56524391] to specialized personnel [@wikidata:Q56524391].

<!-- 1.4.3. Data availability -->
<!-- - 1.4.3.1. As coordinated by the Human Cell Atlas -->
The Human Cell Atlas has a dedicated team for organizing data: the Data Coordination Platform (DCP) [@url:https://data.humancellatlas.org/about] [@wikidata:Q104450645].
The DCP is responsible for tracing the plan for computational interoperability, from the data generators to the consumers.[@wikidata:Q104450645].
The Human Cell Atlas  has its portal for data (<https://data.humancellatlas.org/>) which composes the data repository landscape with other resources, like the Broad Institute Single Cell Portal (<https://singlecell.broadinstitute.org/single_cell>) and the Chan-Zuckerberg Biohub Tabula Sapiens (<https://tabula-sapiens-portal.ds.czbiohub.org/>). 
In addition to its core team, the HCA is poised to grow by community interaction, and states in its opening paper that "As with the Human Genome Project, a robust plan will best emerge from wide-ranging scientific discussions and careful planning".[@wikidata:Q46368626]  
Thus, this project inserts itself among the wide-ranging scientific discussions to improve data - and knowledge - interoperability. 

The highlight of "knowledge" in the last paragraph is meant to stress that raw data _per se_ is not enough to turn the Atlas objectives into reality. 
There is a long way from raw datasets to commonly agreed scientific knowledge. 
And, ultimately, this long way is what allows humanity to take advantage of scientific endeavors.
Currently, the gap between data and knowledge is mostly targeted via the writing and sharing of scientific manuscripts, the _de facto_ currency of exchange of claims about the natural world. 
The Human Cell Atlas Publication Commitee reviews and selects publications that are directly part of the HCA.
A set of publications is, thus, one of the major outputs of the whole endeavor.

The challenge that arises, thus, is one of managing a wealth of information and cast it into useful science.
Experimental articles that analyze thousands of cells pose an overload of information alone. 
Ideally, we would like to understand, remember and make use of every statement produced by the HCA.
As this goal is humanely impossible, we need to develop tools to make the knowledge interoperable with the aid of computers. 
At that point, the challenges of the HCA enter in resonance with the challenges of text-mining, biocuration and literature based discovery, which will be discussed in the chapter <!-- - 1.1. The quest for interoperable knowledge --> of this introduction. 

## Literature Based Discovery, hidden knowledge and text-mining
 <!-- - 1.1. The quest for interoperable knowledge -->
The amount of scholar information vastly outnumbers what single researchers can fathom. 
Nevertheless, the gap between single individuals and the collectively body of knowledge has been widening in 
an accelerated fashion.
The explosion in the number of published articles is leading to a "tsunami of knowlegde",
flooding the scientific literature with rich information. 
That trend became spacially clear during the COVID-19 pandemic, when the huge amount of research published made keeping up with the literature pratically impossible. [@url:https://www.science.org/news/2020/05/scientists-are-drowning-covid-19-papers-can-new-tools-keep-them-afloat @url:https://www.nature.com/articles/d41586-020-03564-y] 
At the same time, articles themselves are becoming denser, as high-throughput (and high-information) technologies
like single-cell RNA-sequencing get cheaper and widely used. 
In practice, thus, too much of the knowledge generated remains unseen by any individual researcher, limiting the reach of science in general.  

 <!-- - 1.1. The quest for interoperable knowledge -->
 <!-- - 1.2.3. Web of Data and Linked Open Data -->
The technological advances, however, are no yet met by equivalent knowledge-handling systems.
Mainstream scientific publication is, nowadays, barely readable by machines.
Articles are written for human consumption, using ambiguous natural language and relying on implicit conventions. 
Tables and data rarely make use of technical standars, such as employing URIs (Uniform Resource Identifiers or encoding information in RDF formats. 
In fact, those standards and their acronyms are foreign for most life scientists (personal observations), despite
being the _de jure_ gold standard for data quality. [@url:https://5stardata.info/en/]
Interconnecting biomedical knowledge is an open challenge of our century, and there is a large way to go
before society can fully benefit from the sum of all knowledge we generate. 

<!--- 1.1.1. Literature Based Discovery, hidden knowledge and text-mining -->
The scientific community has pursue solutions for this tsunami of information from many different angles. 
Narrative reviews, systematic reviews and textbooks compile and synthetize information, providing a layer of processing. 
Biocuration efforts go a step further and transform unstructured information into structured information
in knowledgebases, such as UniProt [@wikidata:Q102383737] and PDB. 
Text-mining apply a range of Natural Language Processing tools to try and extract biological relations,
or provide guidance for biocurators. 
Elaborate knowlegde networks, like the STRING database [@wikidata:Q102383784] and Wikidata[@wikidata:Q87830400], 
combine information from different sources. 

<!-- - 1.1.1.1. Literature Based Discovery (explicitly) --->

The synthesis effort of literature mining is not only an exhibition of the scientific claims in the literature.
Interconnected knowledge provides a way to discover new, implicit knowledge, by applying logical reasoning to a dataset. 
A field denominated Literature Based Discovery [@url:https://en.wikipedia.org/wiki/Literature-based_discovery] dedicates itself to this challenge: make actual discoveries (or at least very strong hypothesis) using as material plainly the existing literature. [@wikidata:Q38371706]
The textbook example of Literature Based Discovery is described by Don Swanson's ABC model: If A is related do B, and B is related to C, then A and C are indirectly related [@wikidata:Q36280460].
In a seminal paper, Swanson showed an hypothesis about using fish oil (A) to treat Raynauld's disease (C), demonstrating that even though the specialized fish-oil (A) literature had shown its association (AB) with a set of blood parameters (B), and the specialized Raynauld's disease literature had show its association (BC) with the same set of parameters (B), the AC link was never made in the literature, despite its seeming obviousness [@wikidata:Q36280460]

<!--- 1.1.1. Literature Based Discovery, hidden knowledge and text-mining -->

Modern advancements of literature-based discovery rely on Natural Language Processing, Machine Learning and Knowledge graphs to make inferences on literature knowledge.
Word embeddings, for example, are leading inference of properties of compounds based on their shared neighbourhood of words (the words before and after their mentiongs) with known compounds, thus making use of latent knowledge in the body of knowledge. [@wikidata:Q91595456]
Other, more explicit approaches, rely on extracted relations embedded in knowledge graphs. As an example, the discovery of new RNA-binding proteins related to Amyotrophic Lateral Sclerosis by analysis of the Watson Drug Discovery gene-disease network. [@wikidata:Q47406275]

Knowledge graphs have a set of characteristics that make then useful for Literature Based Discovery: they represent multiple relations, allow inferences on top of those relations, and provide human understandability at every step, allowing for a dialog between expert humans and computing systems.
The field of biomedical ontologies explores that direction in depth, and the community is building many solutions, widely applicable for the biomedical sciences.

For the Human Cell Atlas Project (as presented in the chapter <!-- 1.4. The challenges of the Human Cell Atlas -->) to maximize its benefit for society, it is arguably important that its knowledge products are inserted into the main route of automated knowledge discovery . 
That implies a task of building knowledge graphs able to deal with it at all layers, including the generated data and metadata, its range of different protocols, and the purified knowledge projects that are enshrined in publications.
Thus, the chapter <!-- - 1.2. Formal representation of knowledge and - 1.3. Knowledge Representation in biology -->  will present challenges and paths for applying literature based discovery on a large scale and with sufficient flexibility to deal with the Human Cell Atlas. 

## Knowledge graphs as tools for interoperability
<!-- - 1.2. Formal representation of knowledge and - 1.3. Knowledge Representation in biology --> 

The classification of biological concepts is at the core of biology. At least since the Aristotelian endeavours to group classes of animals, a good part of the scientific work is to capture concepts into knowledge systems   [@wikidata:Q105870680]. The binomial system for naming species started by Linnaeus and Mendeleiev`s periodic table are likely the two most famous classification systems, but are part of a much larger ecosystem of structuring scientific knowledge. 

<!-- - 1.2.1. Descriptional logic and its historical context -->

On the 20th century, the development of the analytical philosophy of Russel and Wittgenstein and their search for formalizations  [@wikidata:Q105105637] gradually layed the foundations for the the logic of scientific descriptions. Karl Popper and his "The Logic of Scientific Discovery"[@wikidata:Q1868040] was heavily influenced by analytical philosophy, and the field is at the foundation of the "falseability" system of Popper. Less known among life scientists, Tarski's inquiries on what can be considered to be "true" [@wikidata:Q106090790] were also 

The whole movement for formalization of knowledge progressed on the computational end, and at the late 20th century were at the root of the functioning of the World Wide Web, the advent of computational ontologies and large scale knowledge graphs. In this chapter, I will provide an overview of ontologies and knowledge graphs and their use in today's biomedical sciences, alongside its future prospects. 

### The OBO Foundry and biomedical ontologies

An ontology, as used here, is a formal computational representation of reality, which tries to represent each concept (and their relations) as precisely as possible.  [@wikidata:Q105870680]  
Constructing an ontology is a process of selecting and defining terms of interest, selecting and defining relationships of interest and making statements about reality using terms and relationships. 
The Gene Ontology is probably the most well known biomedical ontology; it describes (among other things) different classes of biological process, related to each_other by "is_a" and "part_of relations. [@wikidata:Q104130127] [@wikidata:Q23781406].


The Gene Ontology is part of a much larger effort to formalize concepts across biology: the Open Biomedical and Biological Ontologies (OBO) Foundry. [@wikidata:Q19671692]
Created in 2007, the OBO Foundry is a hub of biomedical ontologies that sets guidelines for the design and construction of high-quality ontologies. 
The initial OBO Foundry united several independent ontologies, like the Cell Ontology (CL), the Disease Ontology (DO) and the Protein Ontology (PRO) under a common framework, a great progress towards interoperability. 
At the same time, the creation of the Relation Ontology (RO) provided a go-to point for relations in biology that could them be reused by different ontologies.

### OWL and ontology languages 
One of the OBO Principles for its ontologies is that they should be resolvable as a "syntactically valid OWL file using the RDF-XML syntax." (http://www.obofoundry.org/principles/fp-002-format.html). 
The OWL Web Ontology Language was introduced as a standard by the W3C consortium in 2004. 
OWL is not a programming language, as it does not instruct computers to perform actions, but an ontology language, which allows computerizable descriptions of the world. 
Furthermore, it is an umbrella ontology language that includes several languages with varying levels of expressivity. 
Generally, more expressive languages can represent more complex ideas, but make computations harder.

Regardless of the sublanguage used by ontology it must be resolvable to an RDF-XML file. RDF stands for Resource Description Framework, another W3C standard built around a graph-based data model (https://www.w3.org/TR/rdf11-concepts/). 
Statements in RDF are triples consisting of 2 nodes (a subject and an object) and a predicate edge connecting the nodes. 
All nodes and edges are represented in RDFs by International Resource Identifiers (IRIs), and there are many ways to lay out those IRIs on a text file to represent triples. 
One of those layouts is the RDF-XML syntax, inspired by the XML markup language. 
Arguably, other syntaxes (interchangeable with RDF-XML) are easier to read for human. 
As an example of an RDF triple, here is how one would represent in the Turtle RDF Syntax, the notion that plasmacytoid dendritic cells are a type of dendritic cells:

```
http://purl.obolibrary.org/obo/CL_0000784  http://www.w3.org/2000/01/rdf-schema#subClassOf  http://purl.obolibrary.org/obo/CL_0000451 .
```
Where http://purl.obolibrary.org/obo/CL_0000784 and  http://purl.obolibrary.org/obo/CL_0000451  are the unique IDs in the Cell Ontology for "plasmocytoid dendritic cells" and dentritic cells, respectively, and http://www.w3.org/2000/01/rdf-schema#subClassOf is the identifier for the "subclass of" relation as defined by the RDF schema. 

A longer explanation of the details of OWL and RDF is outside the scope of this work. 
This brief introduction has a dual goal of introducing the architecture of formal representations and of demonstrating the complexity of the system.
There is a high energy barrier to acquire the knowledge and the technical skills to  engage in ontology building. 
That complexity might be one of the reasons why a very small fraction of the biomedical communities represents data with ontologies and an even smaller fraction engages with ontology building. 

<!-- - 1.2.3. Web of Data and Linked Open Data -->
Even though the Semantic Web  (which ontologies are a part of) spawned with promises of a revolution in the way knowledge is shared, currently, many are of the position that
the Semantic Web is failing to achieve quickly its full extent:
  -  https://twobithistory.org/2018/05/27/semantic-web.html
  -  https://data-mining.philippe-fournier-viger.com/the-semantic-web-and-why-it-failed/
  -  https://www.researchgate.net/post/Why-did-we-fail-in-achiving-the-Semantic-Web-vision
  -  https://blog.diffbot.com/rip-the-semantic-web/


Even with the recent dose of pessimism, the Semantic Web has been receiving a boost of attention recently powered by two large projects: the Google Knowledge Graph and Wikidata. 
The Google Knowledge Graph introduced the Semantic Web _de facto_ in the daily life of users of Google. 
https://blog.google/products/search/introducing-knowledge-graph-things-not/ 

It's underlying structure is similar to the triples in an ontology, but it is less concerned with being logically coherent, and does have strict semantics of the representation. 
In that way, Google Knowledge Graphs can feed on a variety of sources and not crash if there is some logical inconsistency between them.


In other words, a knowledge graph may not be perfectly coherent, as long as the knowledge graph still can provide enough knowledge and reasoning for the approach of interest. 
This lack of formal semantics limits the ability of performing computational reasoning and inference in such knowledge graphs.
On the other hand, knowledge graphs are easier to use, edit and and understand, and so provide an user friendly alternative for computable information with a lower entry barrier. 


Wikidata, the collaborative knowledge graph of the Wikimedia foundation, allows users to contribute with classes and statements, in the same spirit of Wikipedia and share its "epistemic virtues, like power, speed and availability. [@wikidata:Q101955295]
It is powerful because of its large community of contributors. With a community of more than 25,000 active editors (https://www.wikidata.org/wiki/Wikidata:Statistics) and growing, it is able to cover a much wider number of concepts than any user individually. 
It is fast, because one does not need to install any software or ask for permissions to update it: any user can simply do it via a web interface. 
That speed makes it easier for newcomers to join and contribute, in contrast to OBO Foundry ontologies, which require extensive training on semantics and knowledge of Git/GitHub for contributions. 
Finally, the information on Wikidata is available via an user interface, via a SPARQL query service and as large, full-size database dumps, providing full extent reusability. 
The Wikidata model has been so sucessfull that Google decided to migrate its own knowledge base, Freebase, fully into Wikidata.[@wikidata:Q24074986]


* Wikidata as a knowledge graph for the life sciences